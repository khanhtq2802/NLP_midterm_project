# -*- coding: utf-8 -*-
"""Extractive summarization

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NaoT8Vc3LrN8ypxMYHZBqiWSdRL11x2K

Data preprocessing
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/archive.zip

import os
import csv
import random

def read_files_from_folder(folder_path):
    file_list = []
    for root, dirs, files in os.walk(folder_path):
        for file_name in files:
            if file_name.endswith('.txt'):
                file_path = os.path.join(root, file_name)
                file_list.append(file_path)
    return file_list

def split_dataset(dataset_folder, train_ratio):
    # Tạo các danh sách lưu trữ đường dẫn của tập tin bài báo và tóm tắt
    article_files = read_files_from_folder(os.path.join(dataset_folder, 'News Articles'))
    summary_files = read_files_from_folder(os.path.join(dataset_folder, 'Summaries'))

    # Xáo trộn danh sách file
    combined_files = list(zip(article_files, summary_files))
    # random.shuffle(combined_files)
    article_files, summary_files = zip(*combined_files)

    # Tính toán số lượng file để chia thành tập train và test
    num_files = len(article_files)
    num_train = int(num_files * train_ratio)
    num_test = num_files - num_train

    # Tạo các file CSV cho tập train và test
    with open('train.csv', 'w', newline='', encoding='utf-8') as train_file, open('test.csv', 'w', newline='', encoding='utf-8') as test_file:
        train_writer = csv.writer(train_file)
        test_writer = csv.writer(test_file)

        # Ghi header cho các file CSV
        train_writer.writerow(['Article', 'Summary'])
        test_writer.writerow(['Article', 'Summary'])

        # Ghi dữ liệu vào file CSV tập train
        for i in range(num_train):
            article = read_text_from_article(article_files[i])
            summary = read_text_from_summary(summary_files[i])
            train_writer.writerow([article, summary])

        # Ghi dữ liệu vào file CSV tập test
        for i in range(num_train, num_files):
            article = read_text_from_article(article_files[i])
            summary = read_text_from_summary(summary_files[i])
            test_writer.writerow([article, summary])

def read_text_from_article(file_path):
    with open(file_path, 'r', encoding='latin-1') as file:
        text = file.read()
        index = text.find('\n')
        text = text[index + 2:]
    return text

def read_text_from_summary(file_path):
    with open(file_path, 'r', encoding='latin-1') as file:
        text = file.read()
    return text

# Chia dataset với tỷ lệ train:test là 0.8:0.2
split_dataset('/content/BBC News Summary', 0.8)

"""TF-IDF based method"""

import nltk
nltk.download('punkt')  # punkt tokenizer for sentence tokenization
nltk.download('stopwords')  # list of stop words, such as 'a', 'an', 'the', 'in', etc, which would be dropped

# importing the required libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import sent_tokenize
from heapq import nlargest
import numpy as np
from numpy.linalg import norm

def generate_summary(text, n):
    # Tokenize the text into individual sentences
    sentences = sent_tokenize(text)

    # Create the TF-IDF matrix
    vectorizer = TfidfVectorizer(stop_words='english')
    # vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(sentences).toarray()

    # Compute the cosine similarity between each sentence and the document
    sentence_scores = []
    for sentence in tfidf_matrix:
      cosine_similarity = np.dot(tfidf_matrix,sentence)/(norm(tfidf_matrix, axis=1)*norm(sentence))
      score = sum(cosine_similarity) / len(cosine_similarity)
      sentence_scores.append(score)

    # Select the top n sentences with the highest scores
    summary_sentences = nlargest(n, range(len(sentence_scores)), key=sentence_scores.__getitem__)

    summary_tfidf = ' '.join([sentences[i] for i in sorted(summary_sentences)])

    #return summary_tfidf
    return summary_tfidf

# in case it's not installed onto your system.
! pip install rouge

import rouge
from rouge import Rouge
# a defined function called evaluate_rouge taking two arguments,
# one being reference text and the other summary text,
# and uses the ROUGE metric to evaluate the quality of the summary text compared to the reference text.
# The function uses the rouge library to compute the ROUGE scores and returns the F1 score of the ROUGE-1 metric.
def evaluate_rouge1(reference_text, summary_text):
  rouge = Rouge()
  scores = rouge.get_scores(reference_text, summary_text)
  return scores[0]['rouge-1']['f']

def evaluate_rouge2(reference_text, summary_text):
  rouge = Rouge()
  scores = rouge.get_scores(reference_text, summary_text)
  return scores[0]['rouge-2']['f']

def evaluate_rouge3(reference_text, summary_text):
  rouge = Rouge()
  scores = rouge.get_scores(reference_text, summary_text)
  return scores[0]['rouge-l']['f']

eval1 = []
eval2 = []
eval3 = []
with open("/content/test.csv", 'r', encoding='utf-8') as file:
  reader = csv.reader(file)
  for row in reader:
    if len(row) == 2:
      continue
    text = row[1]  # Lấy giá trị từ cột đầu tiên
    reference_summary = row[2]  # Lấy giá trị từ cột thứ hai

    # Generate summary using frequency-based/TF-IDF approach
    n = len(sent_tokenize(reference_summary))
    summary = generate_summary(text, n)

    # Evaluate the summary using ROUGE
    rouge_score1 = evaluate_rouge1(reference_summary, summary)
    rouge_score2 = evaluate_rouge2(reference_summary, summary)
    rouge_score3 = evaluate_rouge3(reference_summary, summary)
    eval1.append(rouge_score1)
    eval2.append(rouge_score2)
    eval3.append(rouge_score3)
score1 = sum(eval1) / len(eval1)
score2 = sum(eval2) / len(eval2)
score3 = sum(eval3) / len(eval3)
print(score1, score2, score3)

"""SVM"""

!python -m spacy download en_core_web_lg

from sklearn import svm
import spacy

# Tải mô hình "en_core_web_lg"
nlp = spacy.load('en_core_web_lg')

# Chuẩn bị dữ liệu huấn luyện và nhãn
X = []
y = []
with open("/content/train.csv", 'r', encoding='utf-8') as file:
  reader = csv.reader(file)
  for row in reader:
    text = row[0]  # Lấy giá trị từ cột đầu tiên
    reference_summary = row[1]  # Lấy giá trị từ cột thứ hai
    doc = nlp(text)
    document_embedding = doc.vector
    for sentence in sent_tokenize(text):
      sent = nlp(sentence)
      sent_embedding = sent.vector
      x = sent_embedding + document_embedding
      X.append(x)
      if sentence in reference_summary:
        y.append(1)
      else:
        y.append(0)

# Xây dựng mô hình SVM
model = svm.SVC(kernel='linear')
model.fit(X, y)

#Test
eval1 = []
eval2 = []
with open("/content/test.csv", 'r', encoding='utf-8') as file:
  reader = csv.reader(file)
  for row in reader:
    summary = ''
    text = row[0]  # Lấy giá trị từ cột đầu tiên
    reference_summary = row[1]  # Lấy giá trị từ cột thứ hai
    doc = nlp(text)
    document_embedding = doc.vector
    for sentence in sent_tokenize(text):
      sent = nlp(sentence)
      sent_embedding = sent.vector
      x = sent_embedding + document_embedding
      y = model.predict(x)
      if y == 1:
        summary += sentence
    rouge_score1 = evaluate_rouge1(reference_summary, summary)
    rouge_score2 = evaluate_rouge2(reference_summary, summary)
    rouge_score3 = evaluate_rouge3(reference_summary, summary)
    eval1.append(rouge_score1)
    eval2.append(rouge_score2)
    eval3.append(rouge_score3)
score1 = sum(eval1) / len(eval1)
score2 = sum(eval2) / len(eval2)
score3 = sum(eval3) / len(eval3)
print(score1, score2, score3)